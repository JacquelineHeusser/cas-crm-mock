# RAG-Chatbot mit Supabase Vector: Implementierungsanleitung

Diese Anleitung beschreibt die Implementierung eines RAG-basierten Chatbots mit Zugriff auf PDF-Dokumente und CRM-Daten.

> **‚ö†Ô∏è Voraussetzung:**
> Diese Anleitung baut auf der [LLM_API_IMPLEMENTATION_GUIDE.md](./LLM_API_IMPLEMENTATION_GUIDE.md) auf.
> Die dort beschriebenen Components (`app/lib/ai.ts`, Environment Variables, Dependencies) werden vorausgesetzt.

## üìã Inhaltsverzeichnis

1. [Architektur-√úbersicht](#architektur-√ºbersicht)
2. [Technologie-Stack](#technologie-stack)
3. [Dependencies](#dependencies)
4. [Dateistruktur](#dateistruktur)
5. [Implementierungs-Reihenfolge](#implementierungs-reihenfolge)
6. [Voraussetzungen](#voraussetzungen)
7. [Automatisierte Tests](#automatisierte-tests)
8. [Manuelle Tests](#manuelle-tests)

---

## Architektur-√úbersicht

Der RAG-Chatbot kombiniert Vektor-Suche mit LLM-generierten Antworten:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CHATBOT UI                                   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Chat Interface (app/chatbot/page.tsx)                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Message Input                                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Chat History                                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Source Citations (PDF Links, Customer/Event Links)          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚îÇ Query
                       ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   /api/chatbot/query (Route Handler)             ‚îÇ
        ‚îÇ                                                   ‚îÇ
        ‚îÇ   1. Create embedding for user query             ‚îÇ
        ‚îÇ   2. Search vector DB (pdfs + crm data)          ‚îÇ
        ‚îÇ   3. Retrieve top-k similar chunks               ‚îÇ
        ‚îÇ   4. Build context with sources                  ‚îÇ
        ‚îÇ   5. Call LLM with RAG prompt                    ‚îÇ
        ‚îÇ   6. Return answer + citations                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                  ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  Supabase      ‚îÇ   ‚îÇ  LLM Service      ‚îÇ
          ‚îÇ  Vector DB     ‚îÇ   ‚îÇ  (lib/ai.ts)      ‚îÇ
          ‚îÇ  (pgvector)    ‚îÇ   ‚îÇ                   ‚îÇ
          ‚îÇ                ‚îÇ   ‚îÇ  - Chat Model     ‚îÇ
          ‚îÇ  - pdf_chunks  ‚îÇ   ‚îÇ  - Embeddings     ‚îÇ
          ‚îÇ  - crm_chunks  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
                    ‚îÇ                  ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ   LangChain (Text Processing)          ‚îÇ
          ‚îÇ   - RecursiveCharacterTextSplitter     ‚îÇ
          ‚îÇ   - Text Chunking mit Overlap          ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Komponentenbeschreibung

| Komponente | Beschreibung |
|------------|-------------|
| **Chat Interface** | React-basierte UI-Komponente f√ºr Benutzereingaben, Chatverlauf und Quellenangaben |
| **Route Handler** | Next.js API-Route, die den RAG-Workflow orchestriert |
| **Supabase Vector DB** | PostgreSQL-Datenbank mit pgvector-Extension f√ºr effiziente Vektorsuche |
| **LLM Service** | Abstraktionsschicht (`lib/ai.ts`) f√ºr OpenAI/Together.ai API-Calls |
| **LangChain** | Python/JS-Library f√ºr LLM-Anwendungen; hier genutzt f√ºr Text-Splitting |
| **pdf_chunks** | Tabelle mit Text-Fragmenten aus PDFs samt Embeddings |
| **crm_chunks** | Tabelle mit serialisierten CRM-Daten (Kunden, Events) samt Embeddings |

### Wichtige RAG-Konzepte

> **RAG (Retrieval-Augmented Generation)** kombiniert Information Retrieval mit generativer KI:
> Statt nur auf das Wissen des LLMs zu vertrauen, werden relevante Dokumente aus einer Datenbank
> abgerufen und als Kontext mitgegeben. So kann das LLM faktisch korrekte Antworten auf Basis
> aktueller, unternehmensspezifischer Daten liefern.

| Begriff | Erkl√§rung |
|---------|----------|
| **Embedding** | Ein Vektor (Array von Zahlen), der die semantische Bedeutung eines Textes repr√§sentiert. √Ñhnliche Texte haben √§hnliche Vektoren (kleiner Abstand im Vektorraum). |
| **Chunking** | Zerlegung grosser Dokumente in kleinere Text-Fragmente (Chunks), damit jedes Fragment ein eigenes Embedding erh√§lt. Typische Chunk-Gr√∂sse: 500‚Äì1500 Zeichen. |
| **Overlap** | √úberlappung zwischen aufeinanderfolgenden Chunks (z.B. 200 Zeichen), um Kontext an den Chunk-Grenzen nicht zu verlieren. |
| **Serialisierung** | Umwandlung strukturierter Daten (z.B. Customer-Objekt) in lesbaren Text, der dann embedded werden kann. |
| **Similarity Search** | Suche nach Vektoren mit geringstem Abstand zum Query-Vektor (Cosine Similarity). |
| **Top-K** | Die K relevantesten Ergebnisse einer Vektorsuche. |
| **HNSW-Index** | Effizienter Index-Algorithmus f√ºr approximative Nearest-Neighbor-Suche in hochdimensionalen Vektorr√§umen. |

### RAG Use Cases

**Use Case 1: PDF Knowledge Base**
1. PDFs (Flyer, Lebensl√§ufe, Dokumentationen) im `cas-crm-mock-files` Storage Bucket
2. Bei Upload: PDF ‚Üí Text-Extraction ‚Üí Chunking ‚Üí Embeddings ‚Üí Vector DB
3. Bei Query: Embedding ‚Üí Similarity Search ‚Üí Top PDFs ‚Üí LLM Answer mit PDF-Link

**Use Case 2: CRM Data Knowledge Base**
1. Customer & Event Daten aus Prisma DB
2. Bei manuellem Sync: Data ‚Üí Text-Serialization ‚Üí Embedding ‚Üí Vector DB
3. Bei Query: Embedding ‚Üí Similarity Search ‚Üí Top Entities ‚Üí LLM Answer mit Detail-Link

### Datenfluss (RAG Query):

```
User Query
   ‚Üì
Create Embedding (text-embedding-3-small oder multilingual-e5-large)
   ‚Üì
Vector Similarity Search (Supabase pgvector)
   ‚Üì
Retrieve Top-K Chunks (pdfs + crm)
   ‚Üì
Build RAG Prompt (context + query)
   ‚Üì
LLM Generate Answer
   ‚Üì
Return Answer + Source Citations
```

---

## Technologie-Stack

### Neue Komponenten (zus√§tzlich zu LLM-Guide):

- **Supabase Vector (pgvector)**: Vector Database f√ºr Embeddings
- **@supabase/supabase-js**: JavaScript Client f√ºr Supabase
- **unpdf**: PDF Text Extraction (Server-side, Next.js-kompatibel)
- **langchain**: Text Splitting & Chunking
- **react-markdown**: Markdown-Rendering f√ºr Chatbot-Antworten
- **@tailwindcss/typography**: Prose-Klassen f√ºr formatierte Texte

### Embedding-Modelle:

| Provider | Modell | Native Dims | Sprachen | Empfehlung |
|----------|--------|-------------|----------|------------|
| **Together.ai** | **multilingual-e5-large-instruct** | **1024** | **100+ inkl. Deutsch** | ‚úÖ **Empfohlen!** |
| OpenAI | text-embedding-3-small | 1536 (k√ºrzbar) | 100+ inkl. Deutsch | Alternative |

---

## Dependencies

### Installation

```bash
npm install @supabase/supabase-js unpdf langchain @langchain/textsplitters react-markdown @tailwindcss/typography
```

> **Wichtig**: Verwende `unpdf` statt `pdf-parse`! Die Library `pdf-parse` 2.x verwendet einen Web Worker (`pdfjs-dist`), der serverseitig in Next.js nicht funktioniert.

### package.json Erg√§nzungen

```json
{
  "dependencies": {
    "@supabase/supabase-js": "^2.45.0",
    "unpdf": "^0.12.0",
    "langchain": "^0.2.0",
    "@langchain/textsplitters": "^0.0.3",
    "react-markdown": "^9.0.0",
    "@tailwindcss/typography": "^0.5.0"
  }
}
```

### Environment Variables (.env)

Erg√§nze die bestehende `.env` um:

```bash
# ==================== EMBEDDINGS ====================

# Embedding Provider: 'openai' oder 'together'
# EMPFOHLEN: Together.ai mit multilingual-e5-large-instruct f√ºr Deutsch/Englisch
EMBEDDING_PROVIDER=together

# Modell-Auswahl je nach Provider
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# Multilingual E5: 100 Sprachen, 1024 Dimensionen, optimiert f√ºr Retrieval
TOGETHERAI_EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct

# Embedding Dimensionen (multilingual-e5: 1024 fix, OpenAI: k√ºrzbar)
EMBEDDING_DIMENSIONS=1024

# ==================== VECTOR SEARCH ====================

# Similarity Threshold (0-1, higher = stricter)
# Empfehlung: 0.75 f√ºr multilingual-e5-large-instruct
VECTOR_MATCH_THRESHOLD=0.75

# Max anzahl zur√ºckgegebener Chunks
VECTOR_MATCH_COUNT=5

# Chunk-Gr√∂√üe f√ºr Text-Splitting
PDF_CHUNK_SIZE=1000
PDF_CHUNK_OVERLAP=200
```

---

## Dateistruktur

```
project/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chatbot/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ route.ts              # Chatbot Query API (RAG)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf/route.ts              # PDF Embedding Generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crm/route.ts              # CRM Embedding Sync
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug/route.ts            # Debug: Embedding-Status pr√ºfen
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test-search/route.ts      # Debug: Similarity Search testen
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ files/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ route.ts                  # File-Handler mit Auto-Embedding
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ chatbot/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                      # Chatbot UI Page
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai.ts                         # [BEREITS VORHANDEN] OpenAI/Together.ai Clients
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.ts                 # Embedding Generation Logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector-search.ts              # Vector Search Utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf-processor.ts              # PDF Text Extraction & Chunking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crm-serializer.ts             # CRM Data ‚Üí Text Serialization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supabase-client.ts            # Supabase Client Setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __tests__/                    # Unit Tests
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ embeddings.test.ts        # Embedding Service Tests
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ pdf-processor.test.ts     # PDF Processor Tests
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ crm-serializer.test.ts    # CRM Serializer Tests
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ api/__tests__/                    # API Route Tests
‚îÇ       ‚îî‚îÄ‚îÄ chatbot-query.test.ts         # Chatbot Query Tests
‚îÇ
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ ChatInterface.tsx                 # Chat UI Component
‚îÇ
‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îú‚îÄ‚îÄ schema.prisma                     # [ERWEITERT] + PdfChunk, CrmChunk Models
‚îÇ   ‚îî‚îÄ‚îÄ vector_setup.sql                  # Vector Indizes & RPC Functions
‚îÇ
‚îî‚îÄ‚îÄ .env                                  # Environment Variables
```

---

## Implementierungs-Reihenfolge

### Mensch: Schritt 1 - pgvector Extension aktivieren und .env erg√§nzen

1. **√ñffne Supabase Dashboard:**
   - Gehe zu deinem Projekt
   - Navigiere zu **Database** ‚Üí **Extensions**
   - Suche nach "vector" und klicke auf **Enable**
   - Warte bis Status = "Active"

> **Schema-Hinweis:** Die pgvector-Extension wird standardm√§ssig im Schema `extensions` installiert.
> Du referenzierst den Vektor-Typ dann als `extensions.vector(dims)`. In Prisma mit `Unsupported()`
> funktioniert auch `vector(dims)` direkt, da Prisma die Typen als Raw-SQL behandelt.

2. **.env erg√§nzen:** "Siehe oben Environment Variables (.env)"

### Mensch: Schritt 2 - Prisma Schema erweitern

Erg√§nze `prisma/schema.prisma` am Ende mit den Vector-Tabellen.

> **‚ö†Ô∏è Wichtig: Embedding-Dimensionen festlegen**
>
> Die Vektor-Dimensionen m√ºssen **einmalig vor der Implementierung** festgelegt werden und sind
> danach nicht mehr einfach √§nderbar (alle Embeddings m√ºssten neu generiert werden).
>
> Die Dimensionen h√§ngen vom gew√§hlten Embedding-Modell ab:
> | Provider | Modell | Dimensionen | Sprachen |
> |----------|--------|-------------|----------|
> | **Together.ai** | **multilingual-e5-large-instruct** | **1024** | ‚úÖ 100+ inkl. Deutsch |
> | OpenAI | text-embedding-3-small | 1536 (k√ºrzbar) | ‚úÖ Multilingual |


**Schema-Erweiterung** (`prisma/schema.prisma`):

```prisma
// =====================================================
// VECTOR TABLES F√úR RAG CHATBOT
// =====================================================

model PdfChunk {
  id         String   @id @default(dbgenerated("gen_random_uuid()")) @db.Uuid
  fileId     String   @db.Uuid
  chunkIndex Int      @map("chunk_index")
  content    String   @db.Text
  embedding  Unsupported("vector(1024)")?  // multilingual-e5-large-instruct: 1024 Dimensionen
  tokenCount Int?     @map("token_count")
  createdAt  DateTime @default(now()) @map("created_at") @db.Timestamptz(3)
  updatedAt  DateTime @updatedAt @map("updated_at") @db.Timestamptz(3)

  // Relations
  file File @relation(fields: [fileId], references: [id], onDelete: Cascade)

  @@unique([fileId, chunkIndex])
  @@index([fileId])
  @@map("pdf_chunks")
}

model CrmChunk {
  id         String   @id @default(dbgenerated("gen_random_uuid()")) @db.Uuid
  entityType String   @map("entity_type") @db.VarChar(20)
  entityId   String   @map("entity_id") @db.Uuid
  content    String   @db.Text
  embedding  Unsupported("vector(1024)")?  // multilingual-e5-large-instruct: 1024 Dimensionen
  metadata   Json?
  createdAt  DateTime @default(now()) @map("created_at") @db.Timestamptz(3)
  updatedAt  DateTime @updatedAt @map("updated_at") @db.Timestamptz(3)

  @@unique([entityType, entityId])
  @@index([entityType, entityId])
  @@map("crm_chunks")
}

model File {
  // ... bestehende Felder ...

  // HINWEIS: Keine direkte Relation definiert, da File polymorph ist
  // (kann zu Event, Customer oder Contact geh√∂ren)
  // ABER: PdfChunks haben eine direkte Relation zu File
  pdfChunks PdfChunk[]

  // ... restliche Felder ...
}
```

Dann Schema pushen und Seed-Daten laden:

```bash
npx prisma db push --force-reset
npx prisma db seed
```


### Mensch: Schritt 3 - Vector Indizes und RPC Functions erstellen

**Ziel**: HNSW-Indizes f√ºr schnelle Vektorsuche und RPC-Functions f√ºr Similarity Search erstellen. (ist auch in scripts/vector_setup.sql aufgef√ºhrt)

**SQL im Supabase Dashboard ausf√ºhren**:

```sql
-- 0. pgvector Extension aktivieren
CREATE EXTENSION IF NOT EXISTS vector;

-- 1. HNSW Indizes f√ºr Performance
CREATE INDEX IF NOT EXISTS pdf_chunks_embedding_idx 
ON pdf_chunks USING hnsw (embedding vector_cosine_ops);

CREATE INDEX IF NOT EXISTS crm_chunks_embedding_idx 
ON crm_chunks USING hnsw (embedding vector_cosine_ops);

-- 2. Vector Similarity Search Function (PDF)
CREATE OR REPLACE FUNCTION match_pdf_chunks(
  query_embedding vector(1024),  -- multilingual-e5-large-instruct: 1024 Dimensionen
  match_threshold float DEFAULT 0.3,
  match_count int DEFAULT 5
)
RETURNS TABLE (
  id uuid,
  "fileId" uuid,
  chunk_index int,
  content text,
  similarity float,
  "fileName" text,
  "storagePath" text
)
LANGUAGE sql
STABLE
AS $$
  SELECT 
    pc.id,
    pc."fileId",
    pc.chunk_index,
    pc.content,
    1 - (pc.embedding <=> query_embedding) AS similarity,
    f."fileName",
    f."storagePath"
  FROM pdf_chunks pc
  JOIN files f ON f.id = pc."fileId"
  WHERE pc.embedding IS NOT NULL
    AND 1 - (pc.embedding <=> query_embedding) > match_threshold
  ORDER BY pc.embedding <=> query_embedding ASC
  LIMIT LEAST(match_count, 50);
$$;

-- 3. Vector Similarity Search Function (CRM)
CREATE OR REPLACE FUNCTION match_crm_chunks(
  query_embedding vector(1024),  -- multilingual-e5-large-instruct: 1024 Dimensionen
  match_threshold float DEFAULT 0.3,
  match_count int DEFAULT 5
)
RETURNS TABLE (
  id uuid,
  entity_type text,
  entity_id uuid,
  content text,
  similarity float,
  metadata jsonb
)
LANGUAGE sql
STABLE
AS $$
  SELECT 
    id,
    entity_type,
    entity_id,
    content,
    1 - (embedding <=> query_embedding) AS similarity,
    metadata
  FROM crm_chunks
  WHERE embedding IS NOT NULL
    AND 1 - (embedding <=> query_embedding) > match_threshold
  ORDER BY embedding <=> query_embedding ASC
  LIMIT LEAST(match_count, 50);
$$;

-- 4. Verify Setup
DO $$
BEGIN
  RAISE NOTICE '‚úÖ Vector Search Setup erfolgreich!';
  RAISE NOTICE '   - pgvector Extension: aktiviert';
  RAISE NOTICE '   - HNSW Indizes: erstellt';
  RAISE NOTICE '   - match_pdf_chunks(): bereit';
  RAISE NOTICE '   - match_crm_chunks(): bereit';
END $$;
```

> **Hinweis:** Das SQL muss nur einmalig ausgef√ºhrt werden. Die Indizes und Functions
> bleiben bestehen, auch nach `prisma db push`.

### LLM: Schritt 4 - Supabase Client Setup

**Datei**: `app/lib/supabase-client.ts`

**Ziel**: Zwei Supabase Client-Funktionen bereitstellen:

1. **`getSupabaseAdmin()`**:
   - Nutzt `SUPABASE_SERVICE_ROLE_KEY` (Server-side only!)
   - F√ºr Admin-Operationen (Embedding-Insert, Vector-Search, SQL-Ausf√ºhrung)
   - `createClient()` mit `auth: { autoRefreshToken: false, persistSession: false }`

2. **`getSupabaseClient()`**:
   - Nutzt `NEXT_PUBLIC_SUPABASE_ANON_KEY`
   - F√ºr Client-side oder authenticated User Operations

**Error Handling**: Werfe Error wenn ENV-Variablen fehlen

---

### LLM: Schritt 5 - Embedding Service Layer

**Datei**: `app/lib/embeddings.ts`

**Ziel**: Zentrale Embedding-Generierung mit Provider-Abstraktion

**Funktionen**:

1. **`EMBEDDING_DIMENSIONS`** (Konstante):
   - Mapping: Modell-Name ‚Üí Dimensionen
   - `text-embedding-3-small`: 1536
   - `text-embedding-3-large`: 3072
   - `intfloat/multilingual-e5-large-instruct`: 1024 (**empfohlen f√ºr Deutsch**)


2. **`generateEmbeddingOpenAI(text, model?)`**:
   - Nutzt `getOpenAIClient()` aus `lib/ai.ts`
   - `client.embeddings.create()` mit Newline-Replacement
   - Returniert `number[]`

3. **`generateEmbeddingTogether(text, model?)`**:
   - Analog f√ºr Together.ai

4. **`generateEmbedding(text)`**:
   - Liest `EMBEDDING_PROVIDER` ENV (default: 'openai')
   - W√§hlt entsprechende Modell-ENV (`OPENAI_EMBEDDING_MODEL` oder `TOGETHERAI_EMBEDDING_MODEL`)
   - Delegiert an provider-spezifische Funktion

5. **`generateEmbeddings(texts[])`** (Batch):
   - Analog zu `generateEmbedding`, aber f√ºr Arrays
   - Nutzt Batch-API f√ºr Performance

---

### LLM: Schritt 6 - PDF Text Processor

**Datei**: `app/lib/pdf-processor.ts`

**Ziel**: PDF ‚Üí Text ‚Üí Chunks mit Token-Counts

**Dependencies**: `unpdf`, `@langchain/textsplitters`

> **Wichtig**: Verwende `unpdf` statt `pdf-parse`! Die Library `pdf-parse` 2.x verwendet einen Web Worker (`pdfjs-dist`), der serverseitig in Next.js nicht funktioniert und zu diesem Fehler f√ºhrt:
> ```
> Failed to extract text from PDF: Setting up fake worker failed: "Cannot find module pdf.worker.mjs"
> ```

**Interface**:
```typescript
interface PDFChunk {
  content: string;
  index: number;
  tokenCount: number;
}
```

**Funktionen**:

1. **`extractTextFromPDF(buffer: Buffer)`**:
   - `import { extractText } from 'unpdf';`
   - `const result = await extractText(buffer);`
   - `unpdf` gibt ein Array von Seitentexten zur√ºck: `result.text.join('\n\n')`
   - Try-Catch mit sinnvollem Error

2. **`chunkText(text: string)`**:
   - `RecursiveCharacterTextSplitter` mit `PDF_CHUNK_SIZE` und `PDF_CHUNK_OVERLAP` aus ENV
   - Returniert `PDFChunk[]` mit index und tokenCount (grobe Sch√§tzung: `text.length / 4`)

3. **`processPDF(buffer: Buffer)`**:
   - Kombiniert `extractTextFromPDF` + `chunkText`

---

### LLM: Schritt 7 - CRM Data Serializer

**Datei**: `app/lib/crm-serializer.ts`

**Ziel**: Customer & Event Daten ‚Üí Text f√ºr Embeddings

**Funktionen**:

1. **`serializeCustomer(customer)`**:
   - Erstelle nat√ºrlichen deutschen Text aus Customer-Feldern
   - Format: "Kunde: [Name]. Typ: [Business/Privat]. Branche: [Industry]. ..." 
   - Verwende nur ausgef√ºllte Felder (`.filter(Boolean)`)

2. **`serializeEvent(event)`**:
   - Analog f√ºr Events
   - Datum formatieren mit `toLocaleDateString('de-DE')`
   - Location als JSON.stringify wenn vorhanden

3. **`getAllCustomersForEmbedding()`**:
   - `prisma.customer.findMany()` mit `where: { isArchived: false }`
   - Select nur relevante Felder

4. **`getAllEventsForEmbedding()`**:
   - `prisma.event.findMany()` mit `where: { status: { not: 'ARCHIVED' } }`

---

### LLM: Schritt 8 - Vector Search Utilities

**Datei**: `app/lib/vector-search.ts`

**Ziel**: Similarity Search via Supabase RPC Functions

**Interfaces**:
```typescript
interface PDFSearchResult {
  id: string;
  fileId: string;
  chunkIndex: number;
  content: string;
  similarity: number;
  fileName: string;
  storagePath: string;
}

interface CRMSearchResult {
  id: string;
  entityType: 'customer' | 'event';
  entityId: string;
  content: string;
  similarity: number;
  metadata: any;
}
```

**Funktionen**:

1. **`searchPDFChunks(query: string)`**:
   - Query ‚Üí Embedding via `generateEmbedding()`
   - `supabase.rpc('match_pdf_chunks')` mit embedding, threshold, count aus ENV
   - **Graceful Degradation**: Bei fehlenden RPC-Funktionen leeres Array zur√ºckgeben statt Crash

2. **`searchCRMChunks(query: string)`**:
   - Analog mit `match_crm_chunks`

3. **`searchKnowledgeBase(query: string)`**:
   - `Promise.all()` f√ºr parallele Suche in beiden Tables
   - Returniert `{ pdfResults, crmResults }`

> **‚ö†Ô∏è Graceful Degradation**: Die RPC-Funktionen existieren m√∂glicherweise nicht, wenn das SQL-Setup noch nicht ausgef√ºhrt wurde. Die Vector-Search sollte in diesem Fall leere Ergebnisse zur√ºckgeben und eine Warnung loggen, statt zu crashen:
> ```typescript
> if (error?.code === '42883' || error?.message?.includes('does not exist')) {
>   console.warn('[Vector Search] RPC function not available. Run SQL setup first.');
>   return [];
> }
> ```

**Debug-Endpoints**:

Erstelle zus√§tzlich zwei Debug-Endpoints f√ºr einfachere Fehlersuche:

1. **`app/api/embeddings/debug/route.ts`**: Zeigt Embedding-Status
   - GET: Counts f√ºr `crm_chunks` und `pdf_chunks`
   - Pr√ºft Embedding-Format (pgvector-String vs JSON-stringified)
   - Gibt Empfehlung bei falschem Format
   
2. **`app/api/embeddings/test-search/route.ts`**: Testet Similarity Search
   - GET: `?q=Suchbegriff&threshold=0.3`
   - Zeigt alle Ergebnisse mit Similarity-Scores
   - Hilft beim Finden des optimalen Thresholds

---

### LLM: Schritt 9 - PDF Embedding API Route

**Datei**: `app/api/embeddings/pdf/route.ts`

**Ziel**: API Endpoint f√ºr Embedding-Generierung und -L√∂schung von PDF-Dokumenten.

**Kontext**: Dieser Endpoint orchestriert den kompletten Workflow von PDF-Download √ºber Text-Extraktion bis Embedding-Insert. Er wird automatisch nach jedem PDF-Upload aufgerufen (siehe Schritt 10).
**Route Config**:
- `export const runtime = 'nodejs'` (Edge Runtime nicht geeignet wegen pdf-parse Buffer-Handling)
- `export const maxDuration = 60` (Sekunden - f√ºr gro√üe PDFs mit vielen Seiten)

**POST Handler** (Embedding erstellen):
- **Input Validation**: Zod Schema `RequestSchema = z.object({ fileId: z.string().uuid() })`
- **Flow**:
  1. **File Record holen**: `supabase.from('files').select('*').eq('id', fileId).single()`
     - Bei Fehler oder nicht gefunden: Return 404
  2. **PDF downloaden**: `supabase.storage.from('cas-crm-mock-files').download(file.storage_path)`
     - Convert Blob zu Buffer: `Buffer.from(await pdfData.arrayBuffer())`
     - Bei Fehler: Return 500
  3. **Text extrahieren & chunken**: `await processPDF(buffer)` ‚Üí `PDFChunk[]`
     - Validierung: Wenn `chunks.length === 0`, return 400 (kein Text extrahierbar)
  4. **Embeddings generieren**: `await generateEmbeddings(chunks.map(c => c.content))`
     - **Wichtig**: Batch-Call f√ºr Performance und Kosteneffizienz!
     - Provider wird automatisch aus ENV gew√§hlt
  5. **Insert in DB**: Map chunks zu DB-Records:
     ```typescript
     const chunkRecords = chunks.map((chunk, i) => ({
       file_id: fileId,
       chunk_index: chunk.index,
       content: chunk.content,
       // WICHTIG: pgvector erwartet String-Format "[0.1,0.2,...]", NICHT JSON.stringify()!
       embedding: `[${embeddings[i].join(',')}]`,
       token_count: chunk.tokenCount,
       created_at: new Date().toISOString(),
       updated_at: new Date().toISOString(),
     }));
     ```
     - Insert via: `supabase.from('pdf_chunks').insert(chunkRecords)`
     - **‚ö†Ô∏è KRITISCH**: `JSON.stringify(embedding)` funktioniert NICHT mit pgvector!
- **Response**: `{ success: true, fileId, chunksCreated: chunks.length }`
- **Error Handling**: 
  - Try-Catch Block um gesamten Handler
  - Console-Log f√ºr Debugging
  - Aussagekr√§ftige Error Messages in Response
  - 500 Status bei internen Fehlern

**DELETE Handler** (Embeddings l√∂schen):
- **Input**: Query Parameter `?fileId=uuid` (via `new URL(request.url).searchParams`)
- **Validation**: Return 400 wenn fileId fehlt
- **Flow**: `supabase.from('pdf_chunks').delete().eq('file_id', fileId)`
- **Hinweis**: Automatisches Cascade Delete funktioniert durch Foreign Key Constraint wenn Parent-PDF gel√∂scht wird
- **Response**: `{ success: true }`

---

### LLM: Schritt 10 - PDF Upload Handler erweitern (Auto-Embedding)

**Datei**: `app/api/files/route.ts` (EXISTING FILE - nur POST Handler erweitern!)

**Ziel**: Nach erfolgreichem PDF-Upload automatisch Embeddings generieren, damit PDFs sofort durchsuchbar sind.

**Kontext**: Der bestehende File-Handler unter `app/api/files/route.ts` ist bereits ein **generischer Handler** f√ºr alle Dateiarten (Event-Flyer, Lebensl√§ufe, etc.). Er verwaltet Metadaten mit `entityType` und `fileType`.
Die Embedding-Integration erfolgt zentral in diesem Handler - alle PDFs werden automatisch indiziert, unabh√§ngig davon, ob es ein Event-Flyer oder ein Lebenslauf ist.

**√Ñnderungen am existing POST Handler**:

1. **Nach dem File-Record Insert** - Synchrones Embedding:
   - **Zus√§tzlicher Code NACH** `prisma.file.create()`
   - **Synchrone Verarbeitung** (blockierend, aber zuverl√§ssig):
     ```typescript
     // Route Config f√ºr l√§ngere PDF-Verarbeitung
     export const maxDuration = 60;
     
     // Nach Upload: Embedding direkt generieren (synchron)
     if (mimeType === 'application/pdf') {
       console.log('[Auto-Embedding] Starting PDF embedding for file:', file.id);
       
       try {
         const supabase = getSupabaseAdmin();
         
         // 1. PDF downloaden
         const { data: pdfData } = await supabase.storage
           .from('cas-crm-mock-files')
           .download(storagePath);
         
         // 2. Text extrahieren & chunken
         const buffer = Buffer.from(await pdfData.arrayBuffer());
         const chunks = await processPDF(buffer);
         
         // 3. Embeddings generieren (Batch)
         const embeddings = await generateEmbeddings(chunks.map(c => c.content));
         
         // 4. In DB speichern
         const now = new Date().toISOString();
         const chunkRecords = chunks.map((chunk, i) => ({
           fileId: file.id,
           chunk_index: chunk.index,
           content: chunk.content,
           embedding: `[${embeddings[i].join(',')}]`,
           token_count: chunk.tokenCount,
           created_at: now,
           updated_at: now,
         }));
         
         await supabase.from('pdf_chunks').insert(chunkRecords);
         console.log(`[Auto-Embedding] Created ${chunks.length} chunks`);
       } catch (error) {
         console.error('[Auto-Embedding] Error:', error);
       }
     }
     ```

> **‚ö†Ô∏è Warum synchron statt Fire-and-forget?**
> 
> Fire-and-forget (`fetch().catch()`) funktioniert **nicht** auf Vercel Preview Deployments:
> - Preview-URLs sind durch Vercel Authentication gesch√ºtzt
> - Server-to-Server Calls werden blockiert (SSO-Redirect)
> - Der Container wird nach Response beendet, bevor der fetch ankommt
>
> Die synchrone L√∂sung ist zuverl√§ssiger und funktioniert √ºberall.

2. **Zus√§tzliche Imports erforderlich**:
   ```typescript
   import { getSupabaseAdmin } from '@/app/lib/supabase-client';
   import { processPDF } from '@/app/lib/pdf-processor';
   import { generateEmbeddings } from '@/app/lib/embeddings';
   ```

**User Experience**:
- PDF Upload ‚Üí Response nach Embedding-Generierung (5-30 Sekunden je nach PDF-Gr√∂√üe)
- PDF ist sofort im Chatbot durchsuchbar
- Bei gro√üen PDFs: Loading-Indicator im UI empfohlen

**Error Handling**:
- Try-Catch um Embedding-Logik: Fehler werden geloggt
- Upload-Response wird trotzdem gesendet (File existiert in DB)
- Embedding kann sp√§ter manuell via `/api/embeddings/pdf` nachgeholt werden

---

### LLM: Schritt 11 - CRM Embedding Sync API Route

**Datei**: `app/api/embeddings/crm/route.ts`

**Ziel**: Bulk-Sync aller CRM-Daten zu Vector DB f√ºr initiales Seeding oder periodisches Update.

**Kontext**: Im Gegensatz zu PDFs (die bei Upload synchronisiert werden) m√ºssen CRM-Daten manuell oder via Cron-Job synchronisiert werden, da √Ñnderungen nicht automatisch getriggert werden.

**Route Config**:
- `export const runtime = 'nodejs'`
- `export const maxDuration = 300` (5 Minuten - f√ºr viele Entities)

**POST Handler**:
- **Flow**:
  1. **Customers syncen**:
     - `await getAllCustomersForEmbedding()` ‚Üí Customer Array
     - For each Customer:
       - `serializeCustomer(customer)` ‚Üí Text
       - `await generateEmbedding(text)` ‚Üí Vector
       - `supabase.from('crm_chunks').upsert()` mit `onConflict: 'entity_type,entity_id'`
       - Metadata: `{ displayName, type, email }` f√ºr sp√§tere Display
  2. **Events syncen**:
     - Analog mit `getAllEventsForEmbedding()`
     - Metadata: `{ title, category, startAt }`
- **Upsert Logik**: `onConflict: 'entity_type,entity_id'` aktualisiert existierende Chunks
- **Response**: `{ success: true, customersSynced, eventsSynced }`
- **Logging**: Console-Log mit Anzahl f√ºr Monitoring
- **Error Handling**: Try-Catch, sinnvolle Fehlermeldungen

**Sync-Ausl√∂sung via UI**:

Der CRM-Sync wird √ºber einen Button auf der Chatbot-Page (Schritt 13) ausgel√∂st:

```typescript
// In der Chatbot UI: Button zum Sync triggern
const handleCrmSync = async () => {
  setSyncing(true);
  try {
    const res = await fetch('/api/embeddings/crm', { method: 'POST' });
    const data = await res.json();
    // Toast oder Alert mit Ergebnis: "X Kunden, Y Events synchronisiert"
  } finally {
    setSyncing(false);
  }
};
```

> **Hinweis**: Der Sync muss initial einmal ausgef√ºhrt werden und danach nur bei gr√∂sseren CRM-Daten√§nderungen.

---

### LLM: Schritt 12 - Chatbot Query API (RAG)

**Datei**: `app/api/chatbot/query/route.ts`

**Ziel**: RAG-Query mit Context Building und LLM Generation

**POST Handler**:
- **Input**: `{ query: string }` (max 1000 chars)
- **Flow**:
  1. `searchKnowledgeBase(query)` ‚Üí pdfResults, crmResults
  2. Build Context Strings:
     - PDFs: `"[PDF-1] filename: content\n\n[PDF-2] ..."`
     - CRM: `"[KUNDE-1] name: content\n\n[VERANSTALTUNG-1] ..."`
  3. Build RAG Prompt:
     - **System**: Du bist CRM-Assistent mit Zugriff auf PDFs/Kunden/Events. Zitiere Quellen. Sei ehrlich wenn Info fehlt.
     - **User**: `Kontext:\n{fullContext}\n\n---\n\nFrage: {query}`
  4. `createChatCompletion()` mit `temperature: 0.3` (faktisch!)
  5. Build Citations Array:
     - PDFs ‚Üí `{ type: 'pdf', id: fileId, title: fileName, url: '/api/files/{id}?download=true' }`
     - Customers ‚Üí `{ type: 'customer', id: entityId, title: displayName, url: '/kunden/{id}' }`
     - Events ‚Üí `{ type: 'event', id: entityId, title: title, url: '/veranstaltungen/{id}' }`
  6. Dedupliziere Citations (by id)
  7. **Filtere Citations**: Nur Quellen anzeigen, die tats√§chlich in der Antwort erw√§hnt werden (Title-Matching)
- **Response**:
```typescript
{
  answer: string;
  citations: Citation[];
  contextUsed: boolean;
  sourcesCount: { pdfs, customers, events };
}
```

---

### LLM: Schritt 13 - Chatbot UI Page

**Datei**: `app/chatbot/page.tsx`

**Ziel**: ChatGPT-style Interface mit Citations

**State**:
- `messages: Message[]` mit `{ role: 'user' | 'assistant', content, citations? }`
- `input: string`
- `loading: boolean`

**UI Components**:

1. **Header**:
   - Titel: "ü§ñ CRM Chatbot"
   - Subtitle: "Frage mich alles √ºber Kunden, Events oder PDFs"

2. **Messages Area**:
   - Empty State: Begr√º√üung + Beispiel-Buttons (z.B. "N√§chste Events", "IT-Kunden")
   - Message Bubbles: User (rechts, blau) / Assistant (links, grau)
   - Citations: Liste unter Assistent-Messages mit Icons (FileText, User, Calendar) und Links
   - Loading Indicator: `Loader2` Icon animiert

3. **Input Form**:
   - Text Input + Send Button (mit `Send` Icon)
   - Disabled w√§hrend loading

**Fetch Logic**:
- `POST /api/chatbot/query` mit `{ query: input }`
- Error Handling mit Fallback-Message

**Icons**: `lucide-react` (Send, FileText, User, Calendar, ExternalLink, Loader2, MessageSquare)

**Styling**: DaisyUI (TailwindCSS-Erweiterung mit vorgefertigten Komponenten wie `btn`, `card`, `chat-bubble`)

---

### LLM: Schritt 14 - Chatbot in Sidebar integrieren

**Datei**: `app/components/Sidebar.tsx` (EXISTING FILE - navItems erweitern!)

**Ziel**: Den Chatbot als Navigation-Link in der Sidebar hinzuf√ºgen.

**√Ñnderungen**:

1. **Import hinzuf√ºgen**:
```typescript
import { MessageSquare } from 'lucide-react';
```

2. **navItems Array erweitern**:
```typescript
const navItems: NavItem[] = [
  { href: '/dashboard', label: 'Dashboard', icon: LayoutDashboard },
  { href: '/kunden', label: 'Kunden', icon: Users },
  { href: '/kontakte', label: 'Kontakte', icon: UserCircle },
  { href: '/veranstaltungen', label: 'Veranstaltungen', icon: Calendar, requiresEventManager: true },
  { href: '/chatbot', label: 'Chatbot', icon: MessageSquare },  // NEU
  { href: '/benutzerverwaltung', label: 'Benutzerverwaltung', icon: Shield, requiresAdmin: true },
  { href: '/einstellungen', label: 'Einstellungen', icon: Settings, requiresAdmin: true },
];
```

> **Hinweis**: Der Chatbot ist f√ºr alle Benutzerrollen sichtbar (kein `requiresAdmin` oder `requiresEventManager`).

---

## Voraussetzungen

### Test-Setup

**Was ist Vitest?**
Vitest ist ein Werkzeug, mit dem man automatisch √ºberpr√ºfen kann, ob der eigene Code korrekt funktioniert ‚Äì auch dann, wenn ein LLM die Tests generiert. Es f√ºhrt kleine, klar definierte Pr√ºfungen (Unit Tests) aus, etwa ob eine Funktion das erwartete Ergebnis liefert oder eine Komponente richtig reagiert.

Unit Testing bedeutet, einzelne, abgeschlossene Teile des Codes isoliert zu testen, damit Fehler fr√ºh und gezielt sichtbar werden.

**Dependencies installieren**:
```bash
npm install --save-dev vitest @testing-library/react @testing-library/jest-dom jsdom
```

**Vitest Config** (`vitest.config.ts`):
```typescript
import { defineConfig } from 'vitest/config';
import react from '@vitejs/plugin-react';

export default defineConfig({
  plugins: [react()],
  test: {
    environment: 'jsdom',  // F√ºr React-Komponenten-Tests
    globals: true,          // describe/it/expect global verf√ºgbar
    setupFiles: ['./vitest.setup.ts'],
  },
});
```

**Warum Mocking?**

In Unit Tests wollen wir isolierte Code-Einheiten testen, ohne echte API-Calls zu machen:

- **API-Mocks**: OpenAI/Together.ai API-Calls werden gemockt, um keine echten Kosten zu verursachen
  und Tests schnell und deterministisch zu halten.
- **Prisma-Mocks**: Datenbankzugriffe werden gemockt, um keine echte DB zu ben√∂tigen.
- **Supabase-Mocks**: Vector-Search und Storage-Calls werden gemockt.

> **Hinweis**: F√ºr **manuelle Tests** (siehe unten) verwenden wir die echte Supabase-DB aus `.env`.
> Nur die **automatisierten Unit Tests** nutzen Mocks.

---

## Automatisierte Tests

> **Hinweis**: Die folgenden Tests sollen vom LLM implementiert werden. Jeder Abschnitt beschreibt **WAS** getestet werden soll, nicht den vollst√§ndigen Code.

---

### LLM: Test 1 - Embedding Service Unit Tests

**Datei**: `app/lib/__tests__/embeddings.test.ts`

**Ziel**: Validierung der Embedding-Generierung und Provider-Abstraktion

**Kontext**: Diese Tests pr√ºfen ob die Embedding-Generation korrekt funktioniert, Provider richtig gewechselt werden und Batch-Processing effizient ist.

**Setup**: 
- Testing Framework: Vitest
- Mock OpenAI/Together.ai Clients mit `vi.mock()`
- Mock ENV Variables f√ºr Provider-Switch Tests mit `vi.stubEnv()`

**Test Cases (zu implementieren)**:

#### `generateEmbeddingOpenAI()`
1. **Sollte Vector mit korrekten Dimensionen returnieren**:
   - Mock OpenAI API mit 1536-dim Vector
   - Assert: Result ist `number[]` mit length 1536
   - Assert: Alle Elemente sind numbers

2. **Sollte Newlines aus Text entfernen**:
   - Input mit `\n` Zeichen
   - Assert: API wurde mit replactem Text aufgerufen

#### `generateEmbeddingTogether()`
1. **Sollte Together.ai API aufrufen**:
   - Mock Together.ai Client
   - Assert: Korrekter Modell-Name wird √ºbergeben
   - Assert: Vector mit 768 Dimensionen (m2-bert)

#### `generateEmbedding()` (Unified Interface)
1. **Sollte OpenAI nutzen wenn EMBEDDING_PROVIDER=openai**:
   - Set ENV Variable
   - Assert: OpenAI Client wird aufgerufen
   - Assert: Together Client wird NICHT aufgerufen

2. **Sollte Together.ai nutzen wenn EMBEDDING_PROVIDER=together**:
   - Set ENV Variable  
   - Assert: Together Client wird aufgerufen
   - Assert: OpenAI Client wird NICHT aufgerufen

3. **Sollte default zu OpenAI fallen wenn ENV nicht gesetzt**:
   - Unset EMBEDDING_PROVIDER
   - Assert: OpenAI wird verwendet

#### `generateEmbeddings()` (Batch)
1. **Sollte Array-API nutzen statt mehrere Einzelcalls**:
   - Input: 5 Texte
   - Assert: Nur 1 API Call, nicht 5
   - Assert: `input` Parameter ist Array

2. **Sollte number[][] mit korrekter Anzahl returnieren**:
   - Input: 3 Texte
   - Mock: API returniert 3 Embeddings
   - Assert: Output ist `number[][]` mit length 3
   - Assert: Jedes Element hat korrekte Dimensionen

**Run Command**: `npm test -- embeddings`

---

### LLM: Test 2 - PDF Processor Unit Tests

**Datei**: `app/lib/__tests__/pdf-processor.test.ts`

**Ziel**: Text-Extraktion und Chunking-Logik validieren

**Kontext**: Diese Tests stellen sicher dass PDFs korrekt verarbeitet werden - Text extrahiert, in sinnvolle Chunks aufgeteilt, mit Overlap und Token-Counts.

**Fixtures**: 
- Erstelle `__fixtures__/test.pdf` - Sample PDF mit bekanntem Inhalt (z.B. "Lorem ipsum..." Text, ~2000 chars)
- F√ºr Error-Test: Invaliden Buffer

**Test Cases (zu implementieren)**:

#### `extractTextFromPDF()`
1. **Sollte Text aus PDF extrahieren**:
   - Input: Test-PDF Buffer
   - Assert: Output ist non-empty String
   - Assert: Length > 0

2. **Sollte Error werfen bei ung√ºltigem PDF**:
   - Input: `Buffer.from('not a pdf')`
   - Assert: Wirft Error mit Message "Failed to extract text from PDF"

#### `chunkText()`
1. **Sollte Text in Chunks mit max size aufteilen**:
   - Input: Langer Text (2000 chars)
   - ENV: `PDF_CHUNK_SIZE=500`, `PDF_CHUNK_OVERLAP=100`
   - Assert: Mehrere Chunks entstehen
   - Assert: Jeder Chunk <= 550 chars (mit 10% Toleranz f√ºr Satz-Ende)

2. **Sollte Overlap zwischen Chunks erzeugen**:
   - Input: Text mit erkennbaren Worten
   - Assert: Ende von Chunk[n] erscheint teilweise in Chunk[n+1]
   - Pr√ºfe letztes Wort von Chunk 0 ist in Chunk 1

3. **Sollte tokenCount f√ºr jeden Chunk berechnen**:
   - Assert: `tokenCount` Property existiert
   - Assert: Grobe Sch√§tzung `text.length / 4` stimmt (¬±10%)

4. **Sollte index f√ºr jeden Chunk setzen**:
   - Assert: Chunk[0].index === 0, Chunk[1].index === 1, etc.

#### `processPDF()` (Integration)
1. **Sollte kompletten Workflow durchf√ºhren**:
   - Input: Test-PDF Buffer
   - Assert: Returniert `PDFChunk[]`
   - Assert: Chunks haben `content`, `index`, `tokenCount`

**Run Command**: `npm test -- pdf-processor`

---

### LLM: Test 3 - CRM Serializer Unit Tests

**Datei**: `app/lib/__tests__/crm-serializer.test.ts`

**Ziel**: Konsistenz der Text-Serialisierung pr√ºfen

**Kontext**: Diese Tests stellen sicher dass CRM-Daten in gut lesbaren deutschen Text konvertiert werden, leere Felder gefiltert und Formate korrekt sind.

**Test Cases (zu implementieren)**:

#### `serializeCustomer()`
1. **Sollte alle gef√ºllten Felder inkludieren**:
   - Input: Customer mit allen Feldern gef√ºllt
   - Assert: Output enth√§lt "Kunde:", "Typ:", "Branche:", "E-Mail:", etc.
   - Assert: Alle Input-Werte erscheinen im Output

2. **Sollte leere Felder filtern**:
   - Input: Customer mit nur `displayName` und `type`
   - Assert: Output enth√§lt keine "undefined", "null" Strings
   - Assert: Nur ausgef√ºllte Felder erscheinen

3. **Sollte BUSINESS Typ korrekt √ºbersetzen**:
   - Input: `type: 'BUSINESS'`
   - Assert: Output enth√§lt "Typ: Firmenkunde"

4. **Sollte PRIVATE Typ korrekt √ºbersetzen**:
   - Input: `type: 'PRIVATE'`
   - Assert: Output enth√§lt "Typ: Privatkunde"

#### `serializeEvent()`
1. **Sollte Datum deutsch formatieren**:
   - Input: Event mit `startAt = new Date('2024-12-25')`
   - Assert: Output enth√§lt Datum im Format `dd.mm.yyyy` (Regex: `/\d{1,2}\.\d{1,2}\.\d{4}/`)

2. **Sollte Location als JSON serialisieren**:
   - Input: `location: { venue: 'Kongresshaus', city: 'Z√ºrich' }`
   - Assert: Output enth√§lt "Ort:"
   - Assert: venue und city sind im Output

3. **Sollte Online-Event kennzeichnen**:
   - Input: `isOnline: true`
   - Assert: Output enth√§lt "Online-Veranstaltung"

4. **Sollte Pr√§senz-Event kennzeichnen**:
   - Input: `isOnline: false`
   - Assert: Output enth√§lt "Pr√§senz-Veranstaltung"

5. **Sollte Kapazit√§t und Preis mit Einheiten formatieren**:
   - Input: `capacity: 100`, `price: 199.00`
   - Assert: Output enth√§lt "Kapazit√§t: 100 Personen"
   - Assert: Output enth√§lt "Preis: 199 CHF"

#### `getAllCustomersForEmbedding()`
1. **Sollte nur nicht-archivierte Kunden holen**:
   - Mock Prisma mit Sample Customers
   - Assert: `prisma.customer.findMany` wurde mit `where: { isArchived: false }` aufgerufen
   - Assert: Returniert nur aktive Kunden

2. **Sollte nur relevante Felder selecten**:
   - Assert: Select enth√§lt displayName, type, email, etc.
   - Assert: Keine sensitiven/unn√∂tigen Felder

#### `getAllEventsForEmbedding()`
1. **Sollte keine archivierten Events holen**:
   - Mock Prisma
   - Assert: `where: { status: { not: 'ARCHIVED' } }`

**Run Command**: `npm test -- crm-serializer`

---

### LLM: Test 4 - Vector Search Unit Tests

**Datei**: `app/lib/__tests__/vector-search.test.ts`

**Ziel**: Similarity Search Logik testen

**Kontext**: Diese Tests pr√ºfen ob Vector Search korrekt mit Supabase RPC kommuniziert, parallele Ausf√ºhrung nutzt und Fehler behandelt.

**Setup**: 
- Mock Supabase Client
- Mock `generateEmbedding()` Function

**Test Cases (zu implementieren)**:

#### `searchPDFChunks()`
1. **Sollte RPC mit korrekten Parametern aufrufen**:
   - Mock: Supabase RPC returniert Sample Results
   - Assert: `supabase.rpc` wurde mit `'match_pdf_chunks'` aufgerufen
   - Assert: Parameter enth√§lt `query_embedding` (Array)
   - Assert: `match_threshold` aus ENV (`VECTOR_MATCH_THRESHOLD`)
   - Assert: `match_count` aus ENV (`VECTOR_MATCH_COUNT`)

2. **Sollte Error werfen bei RPC-Fehler**:
   - Mock: RPC returniert `{ data: null, error: {...} }`
   - Assert: Function wirft Error mit Message "Failed to search PDF chunks"

3. **Sollte leeres Array returnieren bei keinen Matches**:
   - Mock: RPC returniert `{ data: [], error: null }`
   - Assert: Result ist leeres Array

4. **Sollte Results mit korrekter Struktur returnieren**:
   - Assert: Jedes Result hat `id`, `fileId`, `content`, `similarity`, `fileName`

#### `searchCRMChunks()`
1. **Sollte match_crm_chunks RPC aufrufen**:
   - Assert: RPC-Name ist `'match_crm_chunks'`
   - Assert: Selbe Parameter-Struktur wie PDF Search

2. **Sollte Results mit entity_type returnieren**:
   - Assert: Results haben `entityType`, `entityId`, `metadata`

#### `searchKnowledgeBase()` (Combined Search)
1. **Sollte beide Searches parallel ausf√ºhren**:
   - Mock beide RPC Calls
   - Assert: `Promise.all` wird verwendet (nicht sequentiell)
   - Performance: Beide Calls sollten ~gleichzeitig starten

2. **Sollte Object mit beiden Results returnieren**:
   - Assert: Result hat Properties `pdfResults` und `crmResults`
   - Assert: Beide sind Arrays

**Run Command**: `npm test -- vector-search`

---

### LLM: Test 5 - RAG Query API Integration Tests

**Datei**: `app/api/__tests__/chatbot-query.test.ts`

**Ziel**: End-to-End Test des RAG Query Endpoints

**Kontext**: Diese Tests pr√ºfen den kompletten RAG-Flow: Vector Search, Context Building, LLM Call, Citation Building.

**Setup**: 
- SuperTest oder Next.js App Router Test Utils
- Mock `searchKnowledgeBase()`
- Mock `createChatCompletion()`

**Test Cases (zu implementieren)**:

#### POST `/api/chatbot/query` - Success Cases
1. **Sollte Antwort mit Citations returnieren bei Context-Match**:
   - Mock: Vector Search findet relevante PDF-Chunks
   - Mock: LLM generiert Antwort
   - POST mit `{ query: 'Test query' }`
   - Assert: Status 200
   - Assert: Body hat `answer` (String)
   - Assert: `citations` Array mit PDF-Citation
   - Assert: `contextUsed: true`
   - Assert: `sourcesCount` korrekt

2. **Sollte ohne Context antworten wenn keine Matches**:
   - Mock: Vector Search returniert leere Arrays
   - Mock: LLM beantwortet aus General Knowledge
   - Assert: `contextUsed: false`
   - Assert: `citations` ist leer
   - Assert: `answer` ist trotzdem vorhanden

3. **Sollte Citations deduplizieren**:
   - Mock: 2 PDF-Chunks von selber Datei
   - Assert: Nur 1 Citation im Result (dedupliziert by fileId)

4. **Sollte verschiedene Citation-Typen kombinieren**:
   - Mock: PDF + Customer + Event Results
   - Assert: Citations enthalten alle 3 Typen
   - Assert: Korrekte URLs f√ºr jeden Typ

#### POST `/api/chatbot/query` - Validation
1. **Sollte 400 returnieren bei zu langem Query**:
   - Input: Query mit 1001+ Zeichen
   - Assert: Status 400
   - Assert: Zod Validation Error

2. **Sollte 400 returnieren bei fehlendem Query**:
   - Input: `{}` (leeres Object)
   - Assert: Status 400

3. **Sollte 400 returnieren bei ung√ºltigem Query-Typ**:
   - Input: `{ query: 123 }` (Number statt String)
   - Assert: Status 400

#### POST `/api/chatbot/query` - Error Handling
1. **Sollte 500 returnieren bei Vector Search Error**:
   - Mock: `searchKnowledgeBase` wirft Error
   - Assert: Status 500
   - Assert: Sinnvolle Error Message

2. **Sollte 500 returnieren bei LLM Error**:
   - Mock: `createChatCompletion` wirft Error
   - Assert: Status 500

**Run Command**: `npm test -- chatbot-query`

---

## Manuelle Tests

> **Voraussetzungen f√ºr alle manuellen Tests**:
> - Dev Server l√§uft: `npm run dev`
> - pgvector Extension aktiviert
> - Prisma Schema gepusht: `npx prisma db push --force-reset`
> - Vector Indizes erstellt: SQL aus `prisma/vector_setup.sql` im Supabase SQL Editor ausgef√ºhrt
> - `.env` vollst√§ndig konfiguriert (inkl. `NEXT_PUBLIC_APP_URL`)

---

### Mensch: Test 1 - End-to-End: Automatisches PDF-Embedding

**Ziel**: Vollst√§ndiger Workflow von Upload bis Chatbot-Query

**Test-Schritte**:

1. **PDF hochladen**:
   - Navigiere zu File-Upload UI
   - Lade Test-PDF hoch (z.B. Produktdokumentation mit spezifischem Inhalt)
   - **Erwartung**: Success-Response sofort

2. **Auto-Embedding verifizieren** (‚è±Ô∏è Warte 10-30 Sekunden):
   - √ñffne Supabase Dashboard ‚Üí `pdf_chunks` Table
   - Filter: `file_id = [UUID des hochgeladenen PDFs]`
   - **Erwartung**: 
     - Mehrere Chunks sichtbar
     - `embedding` column ist nicht NULL
     - `content` enth√§lt Text aus PDF

3. **Chatbot Query testen**:
   - Navigiere zu `/chatbot`
   - Stelle Frage basierend auf PDF-Inhalt
   - Beispiel: "Was steht im Dokument √ºber [spezifisches Thema aus PDF]?"
   - **Erwartung**: 
     - Antwort enth√§lt relevante Info aus PDF
     - Citations zeigen PDF-Name und Link
     - Click auf Citation √∂ffnet PDF

**Success Criteria**:
- ‚úÖ PDF wird automatisch durchsuchbar (kein manueller `/api/embeddings/pdf` Call n√∂tig)
- ‚úÖ Chatbot findet relevante Chunks
- ‚úÖ Citations funktionieren

---

### Mensch: Test 2 - CRM Data Sync

**Ziel**: CRM-Daten in Vector DB synchronisieren

**Test-Schritte**:

1. **CRM-Sync triggern** (via UI):
   - Navigiere zu `/chatbot`
   - Klicke auf den **"CRM-Daten synchronisieren"**-Button (oben rechts)
   - **Erwartung**: Toast-Nachricht mit Anzahl synchronisierter Eintr√§ge

   *Alternativ via curl (f√ºr Debugging):*
   ```bash
   curl -X POST http://localhost:3000/api/embeddings/crm
   ```

2. **Erwartete Response**:
```json
{
  "success": true,
  "customersSynced": 25,
  "eventsSynced": 10
}
```

3. **Verifizierung in DB**:
   - Supabase Dashboard ‚Üí `crm_chunks` Tabelle
   - **Pr√ºfe**:
     - Eintr√§ge mit `entity_type='customer'` vorhanden
     - Eintr√§ge mit `entity_type='event'` vorhanden
     - `embedding` column ist nicht NULL
     - `content` enth√§lt serialisierten Text
     - `metadata` enth√§lt JSON mit `displayName`/`title`

**Success Criteria**:
- ‚úÖ Customers und Events sind synchronisiert
- ‚úÖ Embeddings sind generiert
- ‚úÖ Metadata ist korrekt

---

### Mensch: Test 3 - Chatbot RAG Query Varianten

**Ziel**: Verschiedene Query-Typen testen

**Voraussetzung**: 
- PDFs hochgeladen und eingebettet (Test 1)
- CRM-Daten synchronisiert (Test 2)

**Test-Schritte**:

1. **Navigiere zu** `http://localhost:3000/chatbot`

2. **Test PDF-basierte Frage**:
   - **Query**: "Was steht in Dokument [PDF-Name] √ºber [Thema]?"
   - **Erwartung**:
     - Antwort mit relevantem Inhalt
     - Citation mit PDF-Name
     - Link zu PDF funktioniert

3. **Test CRM-basierte Frage (Events)**:
   - **Query**: "Welche Veranstaltungen finden n√§chsten Monat statt?"
   - **Erwartung**:
     - Antwort listet Events auf
     - Citations mit Event-Namen
     - Links zu Event-Detail-Pages

4. **Test CRM-basierte Frage (Customers)**:
   - **Query**: "Welche Kunden haben wir aus der IT-Branche?"
   - **Erwartung**:
     - Antwort listet Kunden auf
     - Citations mit Kunden-Namen
     - Links zu Customer-Detail-Pages

5. **Test allgemeine Frage (ohne RAG)**:
   - **Query**: "Was ist die Hauptstadt von Deutschland?"
   - **Erwartung**:
     - Direktantwort vom LLM: "Berlin"
     - Keine Citations (kein lokaler Context)
     - `contextUsed: false` in Response

6. **Test kombinierte Frage**:
   - **Query**: "Gibt es Events zum Thema [PDF-Inhalt]?"
   - **Erwartung**:
     - Antwort kombiniert PDF- und Event-Daten
     - Citations von beiden Quellen

**Success Criteria**:
- ‚úÖ Alle Query-Typen funktionieren
- ‚úÖ Citations sind korrekt und Links funktionieren
- ‚úÖ Antworten sind relevant und faktisch korrekt

---

### Mensch: Test 4 - Vector Similarity Search (SQL)

**Ziel**: RPC Functions direkt testen

**Voraussetzung**: Embeddings in DB vorhanden

**Test-Schritte**:

1. **√ñffne Supabase SQL Editor**

2. **Test PDF Similarity Search**:
```sql
SELECT * FROM match_pdf_chunks(
  (SELECT embedding FROM pdf_chunks LIMIT 1)::vector(1024),
  0.5,
  5
);
```

**Erwartung**:
- Results mit `similarity` zwischen 0-1
- Sortiert nach h√∂chster Similarity
- Max 5 Results
- Joined mit `files` table (file_name, storage_path vorhanden)

3. **Test CRM Similarity Search**:
```sql
SELECT * FROM match_crm_chunks(
  (SELECT embedding FROM crm_chunks LIMIT 1)::vector(1024),
  0.5,
  5
);
```

**Erwartung**:
- Results mit `entity_type` und `entity_id`
- `metadata` column enth√§lt JSON
- Similarity Scores korrekt berechnet

4. **Test mit verschiedenen Thresholds**:
```sql
-- Strenger Threshold
SELECT COUNT(*) FROM match_pdf_chunks(
  (SELECT embedding FROM pdf_chunks LIMIT 1)::vector(1024),
  0.9,  -- Hoher Threshold
  50
);

-- Lockerer Threshold
SELECT COUNT(*) FROM match_pdf_chunks(
  (SELECT embedding FROM pdf_chunks LIMIT 1)::vector(1024),
  0.3,  -- Niedriger Threshold
  50
);
```

**Erwartung**: H√∂herer Threshold ‚Üí weniger Results

**Success Criteria**:
- ‚úÖ RPC Functions funktionieren
- ‚úÖ Similarity Scores sind plausibel
- ‚úÖ Joins funktionieren korrekt

---

### Mensch: Troubleshooting mit Debug-Endpoints

Falls der Chatbot keine Ergebnisse liefert, nutze die Debug-Endpoints:

1. **Embedding-Status pr√ºfen**:
   ```
   GET /api/embeddings/debug
   ```
   - Zeigt Anzahl der Chunks in `crm_chunks` und `pdf_chunks`
   - Pr√ºft ob Embeddings im korrekten Format gespeichert sind
   - **H√§ufiger Fehler**: `embedding_format: "JSON-stringified (WRONG!)"` ‚Üí CRM Sync erneut durchf√ºhren

2. **Similarity Search testen**:
   ```
   GET /api/embeddings/test-search?q=TechCorp&threshold=0.3
   ```
   - Zeigt alle Ergebnisse mit Similarity-Scores
   - Hilft beim Finden des optimalen Thresholds
   - **Tipp**: Threshold=0 zeigt ALLE Ergebnisse, sortiert nach Similarity

3. **H√§ufige Probleme**:

   | Problem | Symptom | L√∂sung |
   |---------|---------|--------|
   | Keine Ergebnisse | `crm_results: { count: 0 }` | CRM-Sync durchf√ºhren |
   | RPC fehlt | `function match_crm_chunks does not exist` | SQL-Script ausf√ºhren |
   | Falsches Format | `embedding_format: "JSON-stringified"` | Code pr√ºfen: `[${embedding.join(',')}]` statt `JSON.stringify()` |
   | Vercel-Fehler | `ECONNREFUSED 127.0.0.1:3000` | `VERCEL_URL`-Fallback in `/api/files/route.ts` |

